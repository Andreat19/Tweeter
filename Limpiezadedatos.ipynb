{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de datos tweets en español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carga de archivo\n",
    "df =  pd.read_csv('twitter.txt', sep=\">\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['User - Location'] = df['User - Location'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la clasificación se necesitara limpiar un poco las columnas de tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminar URL's, nombres de usuarios \n",
    "\n",
    "isURL = re.compile(r'http[s]?:// (?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', re.VERBOSE | re.IGNORECASE)\n",
    "isRTusername = re.compile(r'^RT+[\\s]+(@[\\w_]+:)',re.VERBOSE | re.IGNORECASE) #r'^RT+[\\s]+(@[\\w_]+:)'\n",
    "isEntity = re.compile(r'@[\\w_]+', re.VERBOSE | re.IGNORECASE) \n",
    "\n",
    "def clean_tweet(row):\n",
    "    row = isURL.sub(\"\",row)\n",
    "    row = isRTusername.sub(\"\",row)\n",
    "    row = isEntity.sub(\"\",row)\n",
    "    return row\n",
    "\n",
    "df['Tweet'] = df['Tweet'].apply(lambda row:clean_tweet(row))\n",
    "\n",
    "\n",
    "\n",
    "# Eliminamos salto de linea\n",
    "\n",
    "def Sep_a(city):\n",
    "    return city.replace('\\n',\" \").replace('\\r',\" \")  \n",
    "\n",
    "df['Tweet'] = df['Tweet'].apply(lambda x: Sep_a(x))\n",
    "\n",
    "\n",
    "\n",
    "# Limpiando signos de puntuación,emoticones\n",
    "def remove_punctuation ( text ):\n",
    "    return re.sub(r'[^A-Za-z0-9\\s]', \"\", text)\n",
    "\n",
    "def normalize(city):\n",
    "    replacements = (\n",
    "        (\"á\", \"a\"),\n",
    "        (\"é\", \"e\"),\n",
    "        (\"í\", \"i\"),\n",
    "        (\"ó\", \"o\"),\n",
    "        (\"ú\", \"u\"),\n",
    "        (\"ü\", \"u\"),\n",
    "    )\n",
    "    for a, b in replacements:\n",
    "        city = city.replace(a, b).replace(a.upper(), b.upper()).capitalize().strip()\n",
    "    return  city\n",
    "\n",
    "\n",
    "\n",
    "df['Tweet'] = df['Tweet'].apply(lambda x: normalize(x))\n",
    "df['Tweet'] = df['Tweet'].apply(lambda x: remove_punctuation(x))\n",
    "    \n",
    "\n",
    "# Cambiar a minusculas todas las palabras\n",
    "\n",
    "df['Tweet'] = df['Tweet'].apply(lambda x: x.lower())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpiar Ciudades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sep_c(city):\n",
    "    city = city.split(',')\n",
    "    return city[0]\n",
    "\n",
    "def Sep_h(city):\n",
    "    city = city.split('-')\n",
    "    return city[0]\n",
    "\n",
    "def Sep_a(city):\n",
    "    return city.replace('\\n',\" \").replace('\\r',\" \").replace('\\t',\" \")\n",
    "\n",
    "def Sep_e(city):\n",
    "    return city.replace('  ',\" \")\n",
    "\n",
    "#Corregir con errores de tipeo\n",
    "\n",
    "def Medellin(city):\n",
    "    return  re.sub(r'(?is)Medell.+', 'Medellin', city)\n",
    "\n",
    "def Bogota(city):\n",
    "    return  re.sub(r'(?is)Bog.+', 'Bogota', city)\n",
    "\n",
    "def Popayan(city):\n",
    "    return  re.sub(r'(?is)Popay.+', 'Popayan', city)\n",
    "def Ibague(city):\n",
    "    return city.replace('Colombia Ibague Tolima',\"Ibague\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['User - Location'] = df['User - Location'].apply(lambda x : normalize(x))\n",
    "df['User - Location'] = df['User - Location'].apply(lambda x : Sep_c(x))\n",
    "df['User - Location'] = df['User - Location'].apply(lambda x : Sep_h(x))\n",
    "df['User - Location'] = df['User - Location'].apply(lambda x : Sep_e(x))\n",
    "df['User - Location'] = df['User - Location'].apply(lambda x: remove_punctuation(x))\n",
    "df['User - Location'] = df['User - Location'].apply(lambda x : Sep_a(x))\n",
    "df['User - Location'] = df['User - Location'].apply(lambda x : Bogota(x))\n",
    "df['User - Location'] = df['User - Location'].apply(lambda x : Medellin(x))\n",
    "df['User - Location'] = df['User - Location'].apply(lambda x : Popayan(x))\n",
    "df['User - Location'] = df['User - Location'].apply(lambda x : Ibague(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminanando colomnas no necesarias\n",
    "df.drop((['User', 'User - Name', 'User - Description', 'User - Time Zone']),axis = 1, inplace = True)\n",
    "df.dropna(subset= [\"Tweet\"],inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregando Columna Regiones\n",
    "\n",
    "# 1. Función de clasificación\n",
    "\n",
    "def regiones(city):\n",
    "    rgs = {'Bogota': 'Cundinamarca','Medellin':'Antioquia','Envigado':'Antioquia', 'Cali':'Valle del Cauca',\n",
    "           'Barranquilla':'Atlantico', 'Cartagena':'Bolivar', 'Cucuta':'Norte de Santander', 'Villavicencio':'Meta',\n",
    "           'Ibague':'Tolima', 'Bucaramanga':'Santander', 'Popayan':'Cauca', 'Pasto':'Nariño'}\n",
    "    \n",
    "    if city in rgs.keys():\n",
    "        region = rgs.get(city)\n",
    "    else:\n",
    "        region = city\n",
    "    return region\n",
    "\n",
    "# 2. Aplicando lambda\n",
    "df['Regiones'] = df['User - Location'].apply(lambda x : regiones(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar filas con valores Nan en Columna Tweet\n",
    "df = df.dropna(subset= ['Tweet'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargando predicciones\n",
    "Predicts = pd.read_csv('Predicts.csv')\n",
    "df['Sentimiento'] = Predicts['Sentimiento']\n",
    "#Guardar datos para usar\n",
    "df.to_csv('Data_clean.csv',index = False, encoding = 'utf-8' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear datos para la nube de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['a','acá','ahí','al', 'algo','ambos','ante','antes','aquel','aquí','así','aun','aunque','bajo','bastante','cabe','cada','casi','como','con','cual','cuales','cuan','cuando',\n",
    "              'de','dejar','del','desde','donde','dos','el','él','ella','ello','en','encima','entonces','entre','era', 'eramos','eran','eres','es','esa', 'eso', 'eso', 'esas','esos',\n",
    "              'esta', 'estas', 'estos','esto','este','etc','ha','hace', 'haces','junto', 'juntos','la', 'las', 'los', 'lo','más','me','menos','mi','mis','mia', 'mias','mio','mios','misma',\n",
    "              'mismo','ni','os','otra','otro', 'otras','otros','para','pero','por','por qué','porque','primero','pues','que','qué','quizas', 'quiza','se','sr','sra','sres','sta','su', 'sus',\n",
    "              'tan','te','tú','tu','tus','tuya','tuyo', 'tuyas','tuyos','un', 'una', 'unos', 'unas','y', 'mas', 'si', 'le', 'e', 'do', 'son', 'no','1', '2t','nos','001', 'da']\n",
    "\n",
    "def stop_w(row):\n",
    "    \n",
    "    list_row = row.split()\n",
    "    for i in stop_words:\n",
    "        for j in range(len(list_row)-1, -1, -1):\n",
    "            if list_row[j] == i:\n",
    "                    del(list_row[j])\n",
    "    return \" \".join(list_row)\n",
    "            \n",
    "df['tweet_SW'] = df['Tweet'].apply(lambda x:stop_w(x))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_positive = df[df['Sentimiento'] == 'P']\n",
    "New_negative = df[df['Sentimiento'] == 'N']\n",
    "New_neutral = df[df['Sentimiento'] == 'NEU']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion para convertir todas las palabras en string\n",
    "def list_words(column):\n",
    "    list_ = []\n",
    "    r = column.shape[0]\n",
    "    for i in range(0,r):\n",
    "        text = column.iloc[i].split()\n",
    "        for j in text:\n",
    "            list_.append(j)\n",
    "    return \" \".join(list_)\n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#String de palabras filtradas por sentimiento\n",
    "\n",
    "t_positive = list_words(New_positive['tweet_SW'])\n",
    "t_negative = list_words(New_negative['tweet_SW'])\n",
    "t_neutral = list_words(New_neutral['tweet_SW'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generar nube de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "\n",
    "# Cambiar a fondo blanco imagen\n",
    "\n",
    "def transform_white_backgroud(png_path):\n",
    "    picture = Image.open(png_path).convert(\"RGBA\")\n",
    "    image = Image.new(\"RGB\", picture.size, \"WHITE\")\n",
    "    image.paste(picture, (0, 0), picture)\n",
    "    mask = np.array(image)\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x1b23dc57a60>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generando imagenes\n",
    "\n",
    "# 1. Colocar fondo blanco a la imagen\n",
    "mask = transform_white_backgroud(\"Colombia.png\")\n",
    "\n",
    "\n",
    "# 2. Generar imagenes \n",
    "word_cloud_positive = WordCloud(mask=mask, background_color='rgb(38,50,56)', contour_width=1, contour_color='grey', max_words=200, min_font_size=5, collocation_threshold=10,colormap='Pastel1').generate(t_positive)\n",
    "word_cloud_positive.to_file(\"positive_words.png\")\n",
    "\n",
    "word_cloud_negative = WordCloud(mask=mask, background_color='rgb(38,50,56)', contour_width=1, contour_color='grey', max_words=200, min_font_size=5, collocation_threshold=10,colormap='Pastel1').generate(t_negative)\n",
    "word_cloud_negative.to_file(\"negative_words.png\")\n",
    "\n",
    "word_cloud_neutral = WordCloud(mask=mask, background_color='white', contour_width=1, contour_color='grey', max_words=200, min_font_size=5, collocation_threshold=10).generate(t_neutral)\n",
    "word_cloud_neutral .to_file(\"neutral_words.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
